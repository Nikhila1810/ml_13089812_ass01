{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhila1810/ml_13089812_ass01/blob/master/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjobMGRDU225",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNHlFPD5U3dF",
        "colab_type": "text"
      },
      "source": [
        "TITLE: Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality\n",
        "\n",
        "INTRODUCTION:\n",
        "In this research paper, we discuss the Approximate Nearest Neighbors algorithm as part of the machine learning subject. The problem is as followed \"Given a set of n points, P = (p1,p2, . . . ,pn} in some metric space X, pre-process P so as to efficiently answer queries which require ending the point in P closest to a query point q E X\" (Indyk, 1999). So, for this, the author has proposed various approaches to determine the query time of the results from the dataset. Also, there are many applications for pattern recognition and for retrieving the information from the dataset. The author has validated this by various theorems.\n",
        "\n",
        "\n",
        "CONTENT:\n",
        "This paper consists of four sections. First section deals with the introduction and definition of Nearest Neighbor Search (NNS). In this, the author has elaborated a methodology to solve a problem. We have three sub-major sections in the introduction i.e. motivation, previous work and the overview of the results and techniques. So, the second section is the previous work. In this author described the studies related to the current paper and also mentioned about the results of the techniques. Some of these techniques are about the ways to improve the brute force algorithm for the time-bound and also about the exponential query time. The third section is the overview of the result, in which it focuses on the results of the current paper and then the problem related to the point location of equal balls. In this author also described about the akin methodology which is used to store the equal balls in the dictionary in the format of constrained cells (Indyk, 1999).\n",
        "\n",
        "\n",
        "The second section of the research papers focuses on the preliminaries. Wherein the author explained about the definition, and also the point of location of the ball. This section also deals with several other components such as the norm vector, notation of the space and the hamming metric which are crucial in defining the radius of the ball. In the third section, the author has explained about the PLEB - point location in equal balls. It proposes ways to reduce the problems in point location of equal balls. Here the author has compared between the NNS and the PLEB query time and the pre-processing speed. In this section, the author also explained about the ring-cover tree which helps in finding the separator at any point in time. It also includes the various definitions of the cluster, ring separator and also the theorems which rely on the author's point of view. This papers also deal with the various construction procedure and trees that are related to ring-cover by which we explore the ring cover tree in the best way.\n",
        "\n",
        "In section four of the paper, we have many possible techniques to solve the equal ball problem. So, the first one is Elias Bucketing algorithm, which is used to store the ball in the dictionary as per the format is given by the bounded cell. Using the akin method, we can calculate the query time and the hash functionality that is used to store the information about the ball in the hash table. Another technique is Locality-Sensitive Hashing. This is used for the family hash function that implements a static dictionary. At last, the author has discussed the dimension reduction technique. The main purpose of this paper is data pre-processing and the orderly storage of data in order to obtain the querying time.\n",
        "\n",
        "INNOVATION:\n",
        "Nowadays various applications are based on machine learning algorithms. So, the main purpose of this research to identify the closet point to the query point in the dataset. To solve this problem, we have numerous approaches in KNN such as Kd-trees, brute force search etc. This paper was published in 1999. So, by that time we have only a few research papers in which the author has concentrated mostly on papers publishes in the 1960s. So, this research has the highest and vital novelty. When it comes to the structural patterns of the paper, it has a well-defined structure as the author explained about various approaches and gave a good reference so that the reader can be on track. Pre-processing cost polynomial and the query-time polynomial are the two algorithms which are used by the author. And also, the author has explained about the usage of distance metric to identify the resemblance of the objects.\n",
        "\n",
        "In the research paper of 1997s, the author has improved the information retrieval performance, by using the random projection with the classical geometric lemma and then showed the results.\n",
        "Other techniques such as ring cover tree have been implemented in order to reduce the problem related to the creation of data structure while pre-processing. In the means of solving one problem i.e. NNS, the author discovered the other problem which is related to equal balls in PLEB. The author proposed feasible solutions for two techniques by which the novelty of the paper has been increased. The two techniques are Elias Bucketing algorithm and Locality-Sensitive hashing.  The author also proved that the NNS algorithm is most efficient in terms of pre-processing cost.\n",
        "\n",
        "A web document is used to calculate the hamming space and resemblance between the point distance and to identify the pattern (Broder, 1997). This paper also produces the solution for the problems related to the clustering and the locality-sensitive algorithm. This algorithm is more advantageous as it is used to implement various applications.\n",
        "\n",
        "TECHNICAL QUALITY:\n",
        "The quality of various theorems has well presented. So, the author has described the problems related to curse dimensionality. This can be achieved through the pre-processing cost and the querying time. So, the result obtained in the first algorithm has proved to improve the query processing speed when tested on real-world datasets (Gionis, 1997). NNS problem has been addressed and cleared by the latent semantic indexing technique which is used to decrease the dimensions of the text documents (Berry, January 1995).\n",
        "\n",
        "\n",
        "The author also described the implementation techniques of PLEB, which is used to identify the most closet pair. This explains us about the randomized methodologies which can be simple and easy to check the radius of the two points. This paper suggests the new methodology to identify the correct answer in the sublinear time. Using PLEB the dynamic closest problem has the greedy matching application for a given cluster and the data structure with O(n) as the distance computations (Eppstein, 1998)\n",
        "\n",
        "APPLICATIONS AND X-FACTOR:\n",
        "The most motivational part of this work is the author who gave numerous examples on the dimensionality and techniques which are used to reduce the dimensions such as latent\n",
        "semantic indexing. The author has explained about the effects of the curse of dimensionality in the field of machine learning. In this research paper, the author has attempted to solve the problems related to the approximate nearest Neighbor by adopting various methods and techniques and thus found an interesting solution from them. We can also observe the good literate review in the paper i.e. the various studying of other research papers and their feasible solutions to address these problems. On the whole itâ€™s a good research study and vital application in the field of machine learning. This research not only deals with machine learning but also this can be applied in various fields such as data mining, image pattern recognition and text pattern recognition etc. The first study was by Lipton and Dobkin, who proposed the ANN query time as O (2^d log n) and the pre-processing time as O(n^2^d+1). This was the bases for many scientists to research on ANN and they found that notation of ANN, in which the querying time results differ.\n",
        "\n",
        "In this paper, the author made a comparison between the results from two authors has been done i.e. Frankl and Maehara (Frankl, 1988) respectively. From these results, the author has found that the constraints are to be improved as they play a key role in the time bounds exponent of these algorithms. There is one another comparison between ANN and the NNS algorithms. The most fascinating part of the paper is the collaboration of the technology in which the theorems are implemented.\n",
        "\n",
        "PRESENTATION:\n",
        "The paper has the best presentation style as it explains all the components in depth. Content style, technical explanation and the format of the paper very goof. they are simple and effective. An average student also can read the paper and understand it. \n",
        "\n",
        "The quality of various theorems has well presented. So, the author has described the problems related to curse dimensionality. When it comes to the structural patterns of the paper, it has a well-defined structure as the author explained about various approaches and gave a good reference so that the reader can be on track. At last, the author has discussed the dimension reduction technique. The main purpose of this paper is data pre-processing and the orderly storage of data in order to obtain the querying time. This paper suggests the new methodology to identify the correct answer in the sublinear time. \n",
        "\n",
        "The most motivational part of this work is the author who gave numerous examples on the dimensionality and techniques which are used to reduce the dimensions such as latent semantic indexing. The author has explained about the effects of the curse of dimensionality in the field of machine learning. In this research paper, the author has attempted to solve the problems related to the approximate nearest Neighbor by adopting various methods and techniques and thus found an interesting solution from them. There was a logic interrelation between each theorem, and this was consistently explained by the author.\n",
        "\n",
        "REFERENCE:\n",
        "\n",
        "Berry, D. &. (January 1995). 'A Case Study of Latent Semantic Indexing'. U.T Knoxville Technical report.\n",
        "\n",
        "\n",
        "Broder, A. &. (1997). 'Syntactic clustering of the web'. In: Proceedings of The Sixth International World Wide Web Conference, 391-404.\n",
        "\n",
        "\n",
        "Eppstein, D. (1998). 'Fast hierarchical clustering and other application of dynamic closest pairs'. In: Proceedings of the Ninth ACM -SIAM Symposium on Discrete Algorithm.\n",
        "\n",
        "\n",
        "Frankl, P. &. (1988). The Johnson-Lindenstrauss Lemma and the Sphericity of Some Graphs. Journal of Combinatorial Theory, Series B 44, 355-362.\n",
        "\n",
        "\n",
        "Gionis, A. I. (1997). 'Similarity Search in High Dimensions via Hashing'. 518-529.\n",
        "\n",
        "\n",
        "Indyk, P. &. (1999). 'approximate nearest neighbors towards removing the curse of dimensionality'. 1-20.\n",
        "\n",
        "\n"
      ]
    }
  ]
}